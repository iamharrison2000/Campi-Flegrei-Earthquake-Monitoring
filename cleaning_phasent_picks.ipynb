{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54646c79-f307-4947-842d-302ecefb5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5c0e74-bcf0-42a6-9c67-ec0a5303ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "def filter_duplicates(df, time_threshold=0.1):\n",
    "    df['shifted_time'] = df['phase_time'].shift(-1)\n",
    "    # Compute time difference between consecutive rows in seconds\n",
    "    time_diff = (df['shifted_time'] - df['phase_time']).dt.total_seconds()\n",
    "    # Create a shifted version of 'phase_score' to compare with the next row\n",
    "    df['shifted_score'] = df['phase_score'].shift(-1)\n",
    "    # Identify rows to delete based on time difference and score comparison\n",
    "    # Mark rows whe;re the time difference to the next row is <= 0.05 seconds and the next row's score is greater\n",
    "    mask_delete_current = (time_diff <= time_threshold) & (df['shifted_score'] > df['phase_score'])\n",
    "    mask_delete_next = (time_diff <= time_threshold) & (df['shifted_score'] < df['phase_score']).shift(1).fillna(False)\n",
    "    # Mark rows where the time difference to the previous row is <= 0.03 seconds and the current row's score is not greater than the previous row's score\n",
    "    # mask_delete_current = mask_delete_next.shift(1).fillna(False)\n",
    "    # Combine masks to identify all rows to be deleted\n",
    "    mask_to_delete = mask_delete_next | mask_delete_current\n",
    "    # Filter out the rows marked for deletion\n",
    "    filtered_df = df.loc[~mask_to_delete].copy()\n",
    "    # Drop the auxiliary columns\n",
    "    filtered_df.drop(['shifted_time', 'shifted_score'], axis=1, inplace=True)\n",
    "    \n",
    "    print('drop ', len(df) - len(filtered_df)) \n",
    "    return filtered_df\n",
    "\n",
    "def remove_bad_time(traces_df, pick_df, dt, column):\n",
    "    # Compute 'bad_phase_time' by adding 0.01 seconds to 'starttime_formatted'\n",
    "    traces_df['bad_phase_time'] = traces_df[column] + pd.to_timedelta(dt, unit='s')\n",
    "    # Now, we want to remove rows in 'combined_df' where 'phase_time' matches any 'bad_phase_time' in 'info'\n",
    "    # It's more efficient to do this using a merge or join operation than iterating\n",
    "    # Create a DataFrame from 'bad_phase_time' for a merge operation\n",
    "    bad_times_df = traces_df[['bad_phase_time']].drop_duplicates()  # Ensures unique values for efficient merging\n",
    "    \n",
    "    # Merge 'combined_df' with 'bad_times_df' on the condition that matches 'phase_time' to 'bad_phase_time'\n",
    "    # Use an indicator to mark rows that match\n",
    "    merged_df = pick_df.merge(bad_times_df, left_on='phase_time', right_on='bad_phase_time', how='left', indicator=True)\n",
    "    \n",
    "    # Filter out rows that were matched ('both' in the merge indicator)\n",
    "    filtered_df = merged_df[merged_df['_merge'] == 'left_only'].drop(columns=['bad_phase_time', '_merge'])\n",
    "    print('drop ', len(pick_df) - len(filtered_df)) \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40cae3a5-3d3b-4650-a995-a33c6579d8bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "yr = 2024\n",
    "extra = True\n",
    "extra = False\n",
    "if extra:\n",
    "    #### phases from the accelerometers\n",
    "    file_list = glob.glob(f'./phases/phase_seisbench_{yr}_extra/*picks.txt')\n",
    "else:\n",
    "    #### phases from the non accelerometers\n",
    "    file_list = glob.glob(f'./phases/phase_seisbench_{yr}/*picks.txt')\n",
    "starttime = pd.to_datetime(f'{yr}-01-01').tz_localize('utc')\n",
    "endtime = pd.to_datetime(f'{yr+1}-01-01').tz_localize('utc')\n",
    "\n",
    "time_threshold=0.5\n",
    "tolerance = 0.2\n",
    "\n",
    "for file in file_list:\n",
    "    print(file)\n",
    "    # Read each file into a DataFrame\n",
    "    temp_df = pd.read_csv(file, sep=' ', header=None)\n",
    "    # we might picked some files multiples times\n",
    "    temp_df.drop_duplicates(inplace=True)\n",
    "    temp_df.drop([5], axis=1, inplace=True)\n",
    "    temp_df.columns = ['station_full', 'phase_score', 'date', 'phase_time', 'phase_type']\n",
    "    \n",
    "    # Combine date and time columns into a single datetime column and drop the original columns\n",
    "    temp_df['phase_time'] = pd.to_datetime(temp_df['date'] + ' ' + temp_df['phase_time']).dt.tz_localize('utc')\n",
    "    temp_df.sort_values(by='phase_time', inplace=True)\n",
    "    temp_df.drop(['date'], axis=1, inplace=True)\n",
    "    temp_df = temp_df[(temp_df.phase_time>=starttime) & (temp_df.phase_time<=endtime)]\n",
    "    if len(temp_df)==0:\n",
    "        continue\n",
    "    \n",
    "    # Extract station name from the station_full column\n",
    "    temp_df['station'] = temp_df['station_full'].apply(lambda x: x.split('.')[1])\n",
    "    station_name = temp_df.iloc[0]['station']\n",
    "    \n",
    "    if station_name in ['CROS', 'PTMR', 'V0106', 'CQUE']:\n",
    "        temp_df = temp_df[temp_df.phase_score>=0.5]\n",
    "    if station_name in ['POZA', 'POZT', 'POZS', 'NABA', 'POZM', 'NACO', 'BAN', 'NAD', 'NAFG',\n",
    "                        'BAIP', 'POZL', 'POZB', 'POZU', 'BCLI', 'MPCD', 'NAP', 'NAAG']:\n",
    "        temp_df = temp_df[temp_df.phase_score>=0.6]\n",
    "    elif station_name in ['V0102', 'CSTH']:\n",
    "        temp_df = temp_df[temp_df.phase_score>=0.3]\n",
    "    else:\n",
    "        temp_df = temp_df[temp_df.phase_score>=0.3]\n",
    "    \n",
    "    temp_df['station_id'] = temp_df['station_full'].apply(lambda x: f\"{x.split('.')[0]}.{x.split('.')[1]}..{x.split('.')[2][-2:]}\")\n",
    "    temp_df.drop(['station_full'], axis=1, inplace=True)\n",
    "    \n",
    "    # Reorder the columns\n",
    "    if station_name in ['CAAM', 'COLB', 'CREM', 'CPOZ', 'POZA', 'POZT', 'POZS', 'CROS', 'NABA', 'POZM']:\n",
    "        time_tolerance = 0.2\n",
    "    elif station_name in ['CSOB', 'CPIS', 'CPOZ', 'CSTH', 'V0102', 'CSFT']:\n",
    "        station_tolerance = 0.2\n",
    "    else:\n",
    "        time_tolerance = 0.5\n",
    "        \n",
    "    temp_df = temp_df[['station', 'phase_score', 'phase_time', 'phase_type', 'station_id']]\n",
    "    temp_df = filter_duplicates(temp_df, time_threshold=time_threshold)\n",
    "    temp_df = filter_duplicates(temp_df, time_threshold=time_threshold)\n",
    "\n",
    "\n",
    "    traces_df = pd.read_csv(file[:-9]+'trace.txt', sep=' ', header=None)\n",
    "    traces_df.drop_duplicates(inplace=True)\n",
    "    traces_df['st'] = pd.to_datetime(traces_df[0] + ' ' + traces_df[1]).dt.tz_localize('utc')\n",
    "    traces_df['et'] = pd.to_datetime(traces_df[2] + ' ' + traces_df[3]).dt.tz_localize('utc')\n",
    "    traces_df.drop(columns=[0, 1, 2, 3], inplace=True)\n",
    "    \n",
    "    for dt in np.arange(-tolerance, tolerance+0.01, 0.01):\n",
    "        temp_df = remove_bad_time(traces_df, temp_df, dt, column='st')\n",
    "        temp_df = remove_bad_time(traces_df, temp_df, dt, column='et')\n",
    "    \n",
    "    df_list.append(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b805f5dd-8240-4013-9f47-bcedbceeaa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "final_df = pd.concat(df_list, ignore_index=True)\n",
    "final_df.sort_values(by=[\"phase_time\"], inplace=True)\n",
    "final_df.index.name = 'phase_index'\n",
    "\n",
    "final_df = final_df[final_df.station_id != 'IV.CSTH..EH']\n",
    "final_df = final_df[final_df.station_id != 'IV.CROS..EH']\n",
    "final_df = final_df[final_df.station_id != 'IV.CPV..EH']\n",
    "final_df = final_df[~((final_df.station_id == 'IV.CASE..EH')&(final_df.phase_type == 'S'))]\n",
    "\n",
    "final_df = final_df[~((final_df.station_id == 'IV.CFB1..HH') & (final_df.phase_time < pd.to_datetime('2023-6-20').tz_localize('utc')))]\n",
    "final_df = final_df[~((final_df.station_id == 'IV.CFB3..HH') & (final_df.phase_time < pd.to_datetime('2023-1-1').tz_localize('utc')))]\n",
    "\n",
    "print(final_df.station_id.unique())\n",
    "final_df.to_csv(f'./phases/output/{yr}_picks_2503.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b2593e8-818a-4114-9f0f-73155c8c1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_22 = pd.read_csv('./phases/output/2022_picks_2503.csv')\n",
    "df_23 = pd.read_csv('./phases/output/2023_picks_2503.csv')\n",
    "df_24 = pd.read_csv('./phases/output/2024_picks_2503.csv')\n",
    "df_25 = pd.read_csv('./phases/output/2025_picks_2503.csv')\n",
    "df_24_extra = pd.read_csv('./phases/output/2024_picks_2503_extra.csv')\n",
    "df_25_extra = pd.read_csv('./phases/output/2025_picks_2503_extra.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a277642-9c30-452b-9b7c-a566f0536c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_22, df_23, df_24, df_24_extra, df_25, df_25_extra])\n",
    "df.sort_values(by='phase_time', inplace=True)\n",
    "df.phase_index = np.arange(len(df))\n",
    "df.index = np.arange(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6057b584-b132-4d76-b2f7-65350ae30cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./phases/output/2345_picks.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
